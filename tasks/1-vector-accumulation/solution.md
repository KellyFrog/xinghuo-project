将输入写成一个 $n$ 行 $m$ 列的矩阵，分别解决每一列。

那么需要快速解决一个序列求和的问题，假设 GPU 核心线程数为 $w$，那么可以在每个 block 內将相邻的 $w$ 个元素累加，然后递归到 $n/w$ 的子问题。

现在只需要将相邻的 $w$ 个元素累加，先将对应位置存入共享内存，随后每次将后一半加到前一半（期间同步进程避免访问顺序问题），单次运行的总时间是 $O(\log w)$，解决一列的时间是 $T(n)=T(n/w)+O(n\log w/w)=O(n\log w/w)$，总时间 $O(nm\log w/w)$。

